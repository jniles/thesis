% math.tex

\chapter{Analytical Preliminaries}

In this thesis, we endeavor to secure a relationship between chromatin structure and genomic fragility, to help understand the
progression towards disease states.  Unfortunately, the mathematical tools we will leverage, while both appropriate and necessary,
do not flow naturally from one mathematical field.  Hence, the following chapter will feel disjointed.  By the end, we will have
dabbled in linear algebra, numerical optimization, probability and statistics.

\section*{Normalization of Chromatin Contact Maps}

The entry point for chromatin interaction analysis begins with a chromatin \gls{contact map}.  The Hi-C experiment generates a
genomic contact map describing the number of observed contacts between any two genomic regions, the details of which are covered
in Chapter 2.  Contacts are recorded in square matrix $\matr{O}$ by binning the genome into equally sized intervals.  These bins
are ordered by increasing genomic coordinates, from the start of chromosome 1 to the last bin of chromosome Y.  The cell $O_{ij}$
records the number of observed interactions between bins $i$ and $j$.  Depending on the data quality, bin sizes range from tens of
kilobases to megabases.

The first step of data analysis is to remove experimental biases and establish reproducibility.  Several methods address
normalizing chromatin maps.  Tanay and Yaffe were among the first authors to undertake analysing the Hi-C experiment\cite{yaffe2011}.
They perform a robust analysis of the Hi-C assay identifying several sources systematic biases and propose a non-parametric model
to normalize contact maps by focusing on three systematic biases: variable genome mappability, \gls{GC} content, and fragment length.
The mappings achieved using Tanay's methods provide robust reproducibility between replicates and experiments by treating
\gls{trans contacts} and \gls{cis contacts} seperately\cite{yaffe2011}.  However, the analysis is computationally intensive, requiring
the estimation of contact probabilities on matricies for each bias condition.

Other methods exist for normalizing contact maps.  Hu and collegues propose a method `HiCNorm' based on Poisson regression and achieve
9000x speed up compared to Tanay's method\cite{hu2012}.  However, we employ a normalization proceedure, \gls{ICE}, developed by Imakaev
and collegues, due to the availability of its source code for analysis and performance\cite{imakaev2012}.   We will analyze the
algorithm in detail and provide correctness proofs beyond what the authors report in their paper\cite{imakaev2012}.

\subsection*{\gls{ICE}}

Pursuant of the true contact probility for each genomic region, the \gls{ICE} algorithm makes an important observation that bias matrices
determined by Yaffe and Tanay\cite{yaffe2011} can be successfully reproduced ($r = 0.99$) by a product of biases $B_i \times B_j$.  This
leads to the following proposition

\begin{prop}
  Given the assumption of factorizable biases, the expected contact frequency $\varepsilon_{ij}$ for every pair of regions $(i,j)$ can
  be written as $\varepsilon_{ij} = B_{i}B_{j}T_{ij}$, where $B_i$ and $B_j$ are biases and $T_{ij}$ is the sought matrix of relative contact
  probabilities, normalized as $\sum_{i, i \neq j, j \pm 1}T_{ij} = 1$.
\end{prop}

The authors note that this normalization proceedure results in `equal visibility' regions across the entire genome and maps which are
comparable between Hi-C datasets.  They propose the following algorithm to obtain the biases $B_i$ and `true' relative contact probabilities
$T_{ij}$.

\begin{algorithm}[H]
  \KwData{A matrix of observed interactions $O_{ij}$}
  \KwResult{A matrix of relative contact probabilities $T_{ij}$ and bias vector $B_i$}
  initialize $W^{0}_{ij}$; $B^0 = 1$; $k = 1$\;
  \While{not converged}{%
    $S_i = \sum_{j}W^{k}_{ij}$\;
    $\mean{S_i} = \frac{1}{n}\sum_{i = 1}^{n}S_i$\;
    $\Delta{}B^k_i = \frac{S_i}{\mean{S_i}}$\;
    $W^{k+1}_{ij} = \frac{W^k_{ij}}{\Delta{}B^k_i\Delta{}B^k_j}$\;
    $B^{k+1}_i = B^k_i \dot \Delta{}B^k_i$\;
    $k = k + 1$\;
  }
  \caption{Iterative Correction}
\end{algorithm}


% TODO


\section*{Gene Expression Profiling}

The set of genes expressed in a cell, collectively known as the \textit{transcriptome}, provides insight into chromatin accessibility
and cell state.  A gene expressed in $n$ conditions (or $n$ samples) forms vector in $n-$dimensional space.  Similarly, the expression
profile the entire transcriptome for a single sample is a vector in $m-$dimensional space, where $m$ is the number of genes assessed.
We therefore denote an experiment interrogating $n$ genes across $m$ samples as an $m \times n$ matrix $S_{m,n}$.
\[
  S_{m,n} = \left[
    \begin{array}{cccc}
      s_{1,1} & s_{1,2} & \cdots & s_{1,n} \\
      s_{2,1} & s_{2,2} & \cdots & s_{2,n} \\
      \vdots & \vdots & \ddots  & \vdots \\
      s_{m,1} & s_{m,2} & \cdots & s_{m,n}
    \end{array}
  \right]
\]

and a single gene's expression profile is
\[ G_{i} = \begin{pmatrix} g_{i1} & g_{i2} & g_{i3} & \cdots & g_{in} & \end{pmatrix} \]

We wish to compare differences in gene expression profiles for particular, or all, genes between samples.  This amounts to comparing
\textit{distances} between gene expression vectors.  There are several methods for doing so.

\subsection*{Euclidean Distance}

The simplest genomic distance measure is \textbf{Euclidean} distance.  Euclidean distance is a particular \textit{norm}, known as
the 2-norm.  Geometrically, the norm is a measure of `size' or `proximity' in $n-$dimensional space.  The 2-norm for a vector $x$ is
defined as
\[
  \|x\|_{2} = {(|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)}^{\frac{1}{2}}
\]

The distance between two vectors in a vector space can now be found using the triangle inequality on the 2-norm.  As a norm, the Euclidean
norm has some desirable properties, notably, non-negativity, positivity, and a definition of the triangle inequality
$\|x + y\| = \|x\| + \|y\|$~\cite{horn2013}.  Directly from the triangle inequality, we define the Euclidean distance in $n-$dimensions to
be
\[
  D_{Euc}(x,y) = \|x\| - \|y\| = {(\sum_{i = 1}^{n}{(a_i - b_i)}^2)}^{\frac{1}{2}}
\]


\section*{Principal Component Analysis}

The holy grail of data analysis on high-dimensional data is dimensionality reduction --- that is, to find an accurate representation of
the experiment that need not invoke all the dimensions measured.  The benefits of preprocessing the data set into fewer dimensions are
increases in storage capacity and analysis speed.  As experiments such as Hi-C produce large and larger quantities of data, researchers
attempt to find ways to remove redundency, eliminate unneeded parameters and compress datasets.  One of the most popular methods is
called \gls{PCA}\cite{law1987}.

\begin{defn}[Principal Component Analysis]
  A statistical proceedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables.
\end{defn}

In practice, there are two methods used for \gls{PCA}.  The simplest to explain, but more error-prone, is the eigen-decompositon
method\cite{}.  In this proceedure, the eigenvalues of the covarience matrix $\matr{A}\matr{A}^T$ are calculated directly.  However,
because this involves an extra matrix multiplication, numerical errors are more likely to be introduced during large computations.
In practice, \gls{PCA} often derived in conjunction with \gls{SVD} and we will hold to that standard here.

\begin{thm}[Singular Value Decomposition]
  Let $\matr{A} \in M_{n}(\mathbb{R})$ be given. Then there are unitary matrices $\matr{V} \in M_n$ and $\matr{W} \in M_n$, and a square diagonal
  matrix
  \[
    \matr{\Sigma} =
      \begin{bmatrix}
        \sigma_1 &        & 0        \\
                 & \ddots &          \\
        0        &        & \sigma_n \\
      \end{bmatrix}
  \]
  such that $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n$ and $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$.  The parameters $\sigma_1$,
  $\hdots$, $\sigma_n$ are the positive square roots of the decreasingly ordered non-zero eigenvalues of $\matr{A}\matr{A}^*$, which are the
  same as the decreasingly ordered nonzero eigenvalues of $\matr{A}^*\matr{A}$.

\end{thm}

In order to prove that any square matrix $\matr{A} \in M_n(\mathbb{R})$ can be decomposed into singular values, we must first recall some
matrix definitions.

% Hermetian
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{Hermetian} if $\matr{A}^* =  \matr{A}$.  For real matrices, symmetry implies the Hermetian property.
\end{defn}

% normal
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{normal} if $\matr{A}\matr{A}^* = \matr{A}^*\matr{A}$, this is, it commutes with its conjugate transpose.
\end{defn}

% unitary
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{unitary} if $\matr{A}^*\matr{A} = \matr{A}\matr{A}^* = 1$.  Equivalently, $\matr{A}^* = \matr{A}^{-1}$.
\end{defn}

% unitarily similar
\begin{defn}
  Let $\matr{A},\matr{B} \in M_n$ be given.  We say that $\matr{A}$ is \textnormal{unitarily similar} to $\matr{B}$ if there is a unitary
  $\matr{U} \in M_n$ such that $\matr{A} = \matr{U}\matr{B}\matr{U}^*$.
\end{defn}

\begin{proof}[Singular Value Decomposition]
  It should be clear the matrices $\matr{A}\matr{A}^* \in M_n$ and $\matr{A}^*\matr{A} \in M_n$ have the same eigenvalues, and hence, they are
  unitarily similar.  Then there exists a unitary matrix $\matr{U}$ such that $\matr{A}^*\matr{A} = \matr{U}(\matr{A}\matr{A}^*)\matr{U}^*$.  Then

  \[
    {(\matr{UA})}^*(\matr{UA}) =
    \matr{A}^*\matr{U}^*\matr{UA} =
    \matr{A}^*\matr{A} =
    \matr{UA}\matr{A}^*\matr{U}^* =
    \matr{UA}{(\matr{U}\matr{A})}^*
  \]

  so $\matr{UA}$ is normal.  Let $\lambda_1 = \abs{\lambda_1}e^{i\theta_1}, \ldots, \lambda_n = \abs{\lambda_n}e^{i\theta_n}$ be the positive eigenvalues of
  $\matr{UA}$ in decreasing order.  Furthermore, let $\Delta = diag(\lambda_1, \ldots, \lambda_n)$, let $D = diag(e^{i\theta_1}, \ldots, e^{i\theta_n})$,
  let $\Sigma = diag(\abs{\lambda_1}, \ldots, \abs{\lambda_n})$, and let $\matr{X}$ be a unitary matrix such that $\matr{UA} = \matr{X\Delta}\matr{X}^*$.  Then
  $D$ is unitary and

  \[
    \matr{A} = \matr{U}^*\matr{X}\Sigma\matr{D}\matr{X}^* = (\matr{U}^*\matr{X})\Sigma(\matr{D}\matr{X}^*)
  \]

  If we denote $\matr{V} = \matr{U}^*\matr{X}$ and $\matr{W} = \matr{X}\matr{D}^*$, we have our desired factorization, and
  $\sigma_j = \abs{\lambda_j}, j = 1, \ldots, n$.
\end{proof}

A full proof of \gls{SVD} for rectangular matrices can be found in Horn and Johnson\cite{horn2013}. The relationship between
\gls{SVD} and \gls{PCA} follows directly from the definition of \gls{SVD}.

\begin{thm}
  Let $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$ be the \gls{SVD} of an $n \times n$ dimensional matrix $\matr{A}$ and let

  \[
    \matr{C} = \frac{1}{n - 1}\matr{A}^*\matr{A}
  \]

  be the covariance matrix.  The eigenvectors of $\matr{C}$ are the same as the \textnormal{right singular vectors} of
  $\matr{\Sigma}$.
\end{thm}

\begin{proof}
  Compute
  \[
    \matr{A}^*\matr{A} =
    \matr{V\Sigma}\matr{W}^*\matr{W\Sigma}\matr{V}^* =
    \matr{V\Sigma\Sigma}\matr{V}^* =
    \matr{V}\matr{\Sigma}^2\matr{V}^*
  \]

  \[
    \matr{C} = \matr{V}\frac{\matr{\Sigma}^2}{n - 1}\matr{V}^*
  \]

  $\matr{C}$ is symmetric, and unitarily diagnolizable.  Hence, the eigenvectors of the covariance matrix $\matr{C}$ are the same as the
  matrix $\matr{V}$ (right singular vectors) and the eigenvalues of $\matr{C}$ can be computed directly from the singular values
  $\lambda_i = \frac{\sigma_i}{n - 1}$.
\end{proof}

In principal component analysis, we call the ordered eigenvectors \textit{principal components}.  To ascertain the importance of each
component, one must analyze the \gls{eigenspectrum}, usually in the form of a \gls{scree plot}.  Conceptually, the normalized size of a component's
associated eigenvalue conveys the amount of variation that component captures.  A researcher can the choose a suitable subset of components to
analyze, reducing the dimensionality of the dataset the chosen components.
