% math.tex

\chapter{Analytical Preliminaries}

We endeavor to secure a relationship between chromatin structure and fragility that may potentially lead to disease states.  Our
analysis will include normalization of chromosome contact maps, eigenvalue decomposition, and statistical tests for reprodicibility
and significance.

\section*{Normalization of Genomic Contact Maps}

The Hi-C experiment generates a genomic contact map describing the number of observed contacts between any two genomic
regions.  All \textit{double stranded} reads (those which map uniquely to two genomic loci) can be arranged in a square
matrix by binning the genome into evenly spaced bins.  These bins are arranged into an $n \times n$ matrix $O$ in which the
cell $i,j$ represents the number of observed contacts between bins $i$ and $j$.  Depending on the data quality, these bins
may be as small as 10 kilobases or as large as 2 megabases.

In data analysis, the first step is always to normalize the data set to remove experimental biases and establish reproducibiility.
Several methods have been proposed to normalize Hi-C interaction matrices.  Tanay and Yaffe\cite{tanay2011} performed the first
robust analysis of the Hi-C assay, reporting on several systematic biases and proposing a non-parametric model to normalize contact
maps.  Their model has informed all subsequent normalization methods and is therefore worth discussing in to a depth.

Tanay and Yaffe focus on three systematic biases: genome mappability, GC content, and fragment length.



% TODO


\section*{Expression Profiling}

The set of genes expressed in a cell, collectively known as the \textit{transcriptome}, provides insight into chromatin accessibility
and cell state.  A gene expressed in $n$ conditions (or $n$ samples) forms vector in $n-$dimensional space.  Similarly, the expression
profile the entire transcriptome for a single sample is a vector in $m-$dimensional space, where $m$ is the number of genes assessed.
We therefore denote an experiment interrogating $n$ genes across $m$ samples as an $m \times n$ matrix $S_{m,n}$.
\[
  S_{m,n} = \left[
    \begin{array}{cccc}
      s_{1,1} & s_{1,2} & \cdots & s_{1,n} \\
      s_{2,1} & s_{2,2} & \cdots & s_{2,n} \\
      \vdots & \vdots & \ddots  & \vdots \\
      s_{m,1} & s_{m,2} & \cdots & s_{m,n}
    \end{array}
  \right]
\]

and a single gene's expression profile is
\[ G_{i} = \begin{pmatrix} g_{i1} & g_{i2} & g_{i3} & \cdots & g_{in} & \end{pmatrix} \]

We wish to compare differences in gene expression profiles for particular, or all, genes between samples.  This amounts to comparing
\textit{distances} between gene expression vectors.  There are several methods for doing so.

\subsection*{Euclidean Distance}

The simplest genomic distance measure is \textbf{Euclidean} distance.  Euclidean distance is a particular \textit{norm}, known as
the 2-norm.  Geometrically, the norm is a measure of `size' or `proximity' in $n-$dimensional space.  The 2-norm for a vector $x$ is
defined as
\[
  \|x\|_{2} = {(|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)}^{\frac{1}{2}}
\]

The distance between two vectors in a vector space can now be found using the triangle inequality on the 2-norm.  As a norm, the Euclidean
norm has some desirable properties, notably, non-negativity, positivity, and a definition of the triangle inequality
$\|x + y\| = \|x\| + \|y\|$~\cite{horn2013}.  Directly from the triangle inequality, we define the Euclidean distance in $n-$dimensions to
be
\[
  D_{Euc}(x,y) = \|x\| - \|y\| = {(\sum_{i = 1}^{n}{(a_i - b_i)}^2)}^{\frac{1}{2}}
\]

\subsection*{Correlations between Vectors}

\textbf{TODO}

\section*{Iterative Correction of Interaction Maps}

Massive data mining experiments must be cognizant of any inherent biases in the data set and seek to eliminate them prior to performing
meaningful data analysis.  In this thesis, we depend on accurate reporting of contact frequency between genomic loci, which is prone
to a number of experimental, mechanical, and biological biases\cite{dekker2006}.  In order to compare data sets, I employ an algorithm,
iterative correction and eigenvector decomposition (ICE) developed by Imakaev and colleagues\cite{imakaev2012} established on previous
work by Tanay and Yaffe\cite{yaffe2011}.  In the following section, I will describe the motivation for ICE, the algorithm in detail, and
prove convergence properties for the algorithm.

Genome-wide interaction maps form matrix of observed contact frequencies between genomic segments.  Observed contacts are expected to
be noisy do to biases introduced in the experimental procedure and inherent differences in mappability for different genomic regions
(e.g.\ centromeres).

% Why are biases factorizable?

The ICE algorithm decomposes a contact map into the product of true contact frequencies and a set of biases for every region
\[
  O_{ij} = B_{i}B_{j}T_{ij}
\]
with $T_{ij}$ normalized as $\sum_{i,i \neq j,j \pm 1}T_{ij} = 1$ for each region $j$.


\section*{Principal Component Analysis}

The holy grail of data analysis on high-dimensional data is dimensionality reduction --- that is, to find an accurate representation of
the experiment that need not invoke all the dimensions measured.  The benefits of preprocessing the data set into fewer dimensions are
increases in storage capacity and analysis speed.  As experiments such as Hi-C produce large and larger quantities of data, researchers
attempt to find ways to remove redundency, eliminate unneeded parameters and compress datasets.  One of the most popular methods is
called \gls{PCA}\cite{law1987}.

\begin{definition}[Principal Component Analysis]
  A statistical proceedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables.
\end{definition}

In practice, there are two methods often used for \gls{PCA}.  The simplest to explain, but more error-prone, is the eigen-decompositon
method\cite{}.  In this proceedure, the eigenvalues of the covarience matrix $\matr{A}\matr{A}^T$ are calculated directly.  However,
because this involves an extra matrix multiplication, numerical errors are more likely to be introduced during large computations.
In practice, \gls{PCA} often derived in conjunction with \gls{SVD} and we will hold to that standard here.

\begin{theorem}[Singular Value Decomposition]
  Let $\matr{A} \in M_{m,m}$ be given. Then there are unitary matrices $\matr{V} \in M_m$ and $\matr{W} \in M_m$, and a square diagonal
  matrix
  \[
    \matr{\sum} =
      \begin{bmatrix}
        \sigma_1 &        & 0        \\
                 & \ddots &          \\
        0        &        & \sigma_m \\
      \end{bmatrix}
  \]
  such that $\sigma_1 \gte \sigma_2 \gte \hdots \gte \sigma_m$ and $\matr{A} = \matr{V}\matr{\sum}\matr{W}^T$.  The parameters $\sigma_1$,
  $\hdots$, $\sigma_m$ are the positive square roots of the decreasingly ordered non-zero eigenvalues of $\matr{A}\matr{A}^T$.
\end{theorem}

A full proof can be found in Horn and Johnson\cite{horn2013}.  The relationship between \gls{SVD} and \gls{PCA} follows directly from the
definition of \gls{SVD}.

\begin{theorem}
  Let $\matr{A} = \matr{V}\matr{\sum}\matr{W}^T$ be the \gls{SVD} of an $N \times N$ dimensional matrix $\matr{A}$ and let \[C = \frac{1}{N - 1}
    \matr{X}^T\matr{X}\] be the covariance matrix.  The eigenvectors of $\matr{C}$ are the same as the right singular vectors of $\matr{\sum}$.
\end{theorem}

\begin{proof}
\end{proof}


Mathematically speaking, eliminating redundency is equivalent to eliminating correlations between variables.  \gls{PCA} accomplishes this
elimination by performing a linear tranformation of the dataset (a matrix of observations) onto an orthogonal bases of \textit{Principal Components}.
Components are decreasingly ordered by the amount of the original dataset's variance they capture.  The hope is that the majority of
the dataset's variance will be captured in a small number of principal components.  Researchers then seek to explain what variable or
combination of variables are encoded in each principal component.
