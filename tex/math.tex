% math.tex

\chapter{Mathematical Preliminaries}

We endeavor to secure a relationship between chromatin structure and genomic fragility to elucidate the progression towards
disease states.  Unfortunately, the mathematical tools we leverage, while both appropriate and necessary, do not flow
naturally from a single mathematical field.  Therefore, the following chapter will feel disjointed.  By the end, we will have
dabbled in linear algebra, numerical optimization, probability and statistics.

\section*{Normalization of Chromatin Contact Maps}

The entry point for chromatin interaction analysis begins with a chromatin \gls{contact map}.  The Hi-C experiment generates a
genomic contact map describing the number of observed contacts between any two genomic regions, the details of which are covered
in Chapter 2.  Contacts are recorded in square matrix $\matr{O}$ by binning the genome into equally sized intervals and considering
the pairwise interactions between each bin.  Bins are ordered by increasing genomic coordinates, from the first bin of chromosome 1 to
the last bin of chromosome Y.  The cell $O_{ij}$ contains the number of observed interactions between bins $i$ and $j$.  Depending on
data quality, bin sizes range from tens of kilobases to megabases.

The first step of data analysis is \gls{normalization}; that is, remove experimental biases and establish reproducibility.
Several methods exist to normalize contact maps.  Tanay and Yaffe were among the first authors to undertake a statistical analysis of the Hi-C experiment\cite{yaffe2011}.
They identify several sources systematic experimental bias in the Hi-C assay and propose a probabilistic background model representing \gls{GC}
content, fragment length, and genome mappability.  Corrected contact maps are calculated from the experiments based on these contact maps.  The mappings achieved using Tanay
and Yaffe's methods provide robust reproducibility between replicates and experiments by considering \gls{trans contacts} and \gls{cis contacts}
separately\cite{yaffe2011}.  However, their analysis pipeline is computationally intensive, prompting further research into Hi-C analysis.

Other methods exist for normalizing contact maps.  Hu and colleagues propose a method `HiCNorm' based on Poisson regression and achieve
9000x speed up compared to Tanay's method\cite{hu2012}.  Recently, Ay and colleagues provide a statistical method `Fit-Hi-C' that does
not assume a particular underlying statistical distribution, instead normalizing cis contacts based on probabilistic analysis of polymer looping
dynamics\cite{ay2014}.

Imakaev and colleagues propose a normalization and analysis pipeline \gls{ICE}\cite{imakaev2012}.  We employ \gls{ICE} as the normalization
algorithm in this thesis, due to the availability of its source code\footnote{source: \url{http://mirnylab.bitbucket.org/hiclib/}} and good
performance.   We analyze the algorithm in detail.

\subsection*{\glsentryfull{ICE}}

Pursuant of the true contact probability for each genomic region, \gls{ICE} makes a critical observation that bias matrices determined by
Yaffe and Tanay\cite{yaffe2011} can be successfully reproduced ($r = 0.99$) by a product of biases $B_i \times B_j$.  This observation leads
immediately to the following proposition

\begin{prop}
  Given the assumption of factorizable biases, the expected contact frequency $\varepsilon_{ij}$ for every pair of regions $(i,j)$ can
  be written as $\varepsilon_{ij} = B_{i}B_{j}T_{ij}$, where $B_i$ and $B_j$ are biases and $T_{ij}$ is the sought matrix of relative contact
  probabilities, normalized as $\sum_{i, i \neq j, j \pm 1}T_{ij} = 1$.
\end{prop}

The authors note that this normalization procedure results in `equal visibility' regions across the entire genome and maps which are
comparable between Hi-C data sets.  They propose the following algorithm to obtain the biases $B_i$ and `true' relative contact probabilities
$T_{ij}$.

\begin{algorithm}[H]
  \KwData{A matrix of observed interactions $O_{ij}$}
  \KwResult{A matrix of relative contact probabilities $T_{ij}$ and bias vector $B_i$}
  initialize $W^{0}_{ij}$; $B^0 = 1$; $k = 1$\;
  \While{not converged}{%
    $S_i = \sum_{j}W^{k}_{ij}$\;
    $\mean{S_i} = \frac{1}{n}\sum_{i = 1}^{n}S_i$\;
    $\Delta{}B^k_i = \frac{S_i}{\mean{S_i}}$\;
    $W^{k+1}_{ij} = \frac{W^k_{ij}}{\Delta{}B^k_i\Delta{}B^k_j}$\;
    $B^{k+1}_i = B^k_i \dot \Delta{}B^k_i$\;
    \eIf{$Var(B^{k+1}) < threshold$}{%
      converged\;
    }{%
      $k = k + 1$\;
    }
  }
  \caption{Iterative Correction}
\end{algorithm}

It is not apparent that this algorithm is correct or converges.  To gain an intuitive understanding of the solution, let us investigate a
simpler proposition.  Suppose that, instead of defining $O_{ij}$ to be the matrix product $B_{i}B_{j}T_{ij}$, we consider that the counts
in $O_{ij}$ are the expectation of some multi-nomially distributed random variable $X_{ij}$, where $E[X_{ij}] = NB_{i}B_j$ for some
constant $N$ and vector $B$ whose cumulative sum is 1 ($\sum_{i}B_i = 1$).  In this formulation, each of the cells of $O_{ij}$ are
independent and distributed according to $B$.  The count of cell $i,j$ is given by the probability

\[
  \log{p_{ij}} =  c + u_i + u_j - Z
\]

where $c = \log{N}, u_i = \log{B_i}, u_j = \log{B_j}$, and $Z (=c)$ is a normalization factor to ensure that $\sum p = 1$.  This type of
model is called a \gls{log-linear model} or \gls{toric model}.  The problem is now a maximization problem: what are the maximum likelihood parameters
that best fit the model to the observed data?  Luckily, the methods and their convergence properties have been extensively studied in the
literature\cite{fienberg2012}\cite{pachter2005}.

In the realm of statistics, the contact matrix, along with the calculated margins, is called a \gls{contingency table}.  Importantly, assuming
that the margins are positive, and that the matrix cannot be permuted into block diagonal shape, Birch's theorem guarantees that there is a
unique maximum to the likelihood function\cite{pachter2005}.  For full details, consult Prachter and Sturmfels\cite{pachter2005}.  Furthermore,
the marginal values are the \gls{sufficient statistic} of the model.  In other words, the maximum likelihood parameters for this data is given
by the normalized row and column sums of the matrix\cite{pachter2005}.

With the observation that there exists a global maximum of the likelihood function, all that remains is to compute the `true' values by some
process.  One common algorithm is the \gls{EM} algorithm\cite{fuchs1982}.  Imakaev and colleagues employ a simpler algorithm known as \gls{IPF},
developed by Deming and Stephan in 1940, and apparently rediscovered by Imakaev's group\cite{deming1940}.  \gls{IPF} works generally by solving
the \gls{MLE} while leaving the margins ($p_{i+} = \sum_{j}p_{ij}$ and $p_{j+} = \sum_{i}p_{ij}$) fixed.  A proof of convergence for contingency
tables follows from Feinberg's work in algebraic geometry in 1970\cite{feinberg1970}.

Finally, we return to the \gls{toric model} we described above.  Since the log-likelihood function is concave, the \gls{IPF} algorithm first
computes the roots of the partial derivatives of the log-likelihood function and sets them to zero to solve for the global maximum.  Imakaev
and colleagues consider the likelihood function on the Poisson distribution.

\[
  L(B) = \sum_{i,j}O_{ij} \dot (\log{B_i} + \log{B_j}) - NB_{i}B_{j} + C
\]

in which $C$ is a constant.

% TODO

\section*{Principal Component Analysis}

The holy grail of data analysis on high-dimensional data is dimensionality reduction --- that is, to find an accurate representation of
the experiment that need not invoke all the dimensions measured.  The benefits of preprocessing the data set into fewer dimensions are
increases in storage capacity and analysis speed.  As experiments such as Hi-C produce large and larger quantities of data, researchers
attempt to find ways to remove redundancy, eliminate unneeded parameters and compress data sets.  One of the most popular methods is
called \gls{PCA}\cite{law1987}.

\begin{defn}[Principal Component Analysis]
  A statistical procedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables.
\end{defn}

In practice, there are two methods used for \gls{PCA}.  The simplest to explain, but more error-prone, is the eigen-decomposition
method\cite{}.  In this procedure, the eigenvalues of the covariance matrix $\matr{A}\matr{A}^T$ are calculated directly.  However,
because this involves an extra matrix multiplication, numerical errors are more likely to be introduced during large computations.
In practice, \gls{PCA} often derived in conjunction with \gls{SVD} and we will hold to that standard here.

\begin{thm}[Singular Value Decomposition]
  Let $\matr{A} \in M_{n}(\mathbb{R})$ be given. Then there are unitary matrices $\matr{V} \in M_n$ and $\matr{W} \in M_n$, and a square diagonal
  matrix
  \[
    \matr{\Sigma} =
      \begin{bmatrix}
        \sigma_1 &        & 0        \\
                 & \ddots &          \\
        0        &        & \sigma_n \\
      \end{bmatrix}
  \]
  such that $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n$ and $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$.  The parameters $\sigma_1$,
  $\hdots$, $\sigma_n$ are the positive square roots of the decreasingly ordered non-zero eigenvalues of $\matr{A}\matr{A}^*$, which are the
  same as the decreasingly ordered nonzero eigenvalues of $\matr{A}^*\matr{A}$.

\end{thm}

In order to prove that any square matrix $\matr{A} \in M_n(\mathbb{R})$ can be decomposed into singular values, we must first recall some
matrix definitions.

% Hermetian
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{Hermetian} if $\matr{A}^* =  \matr{A}$.  For real matrices, symmetry implies the Hermetian property.
\end{defn}

% normal
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{normal} if $\matr{A}\matr{A}^* = \matr{A}^*\matr{A}$, this is, it commutes with its conjugate transpose.
\end{defn}

% unitary
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{unitary} if $\matr{A}^*\matr{A} = \matr{A}\matr{A}^* = 1$.  Equivalently, $\matr{A}^* = \matr{A}^{-1}$.
\end{defn}

% unitarily similar
\begin{defn}
  Let $\matr{A},\matr{B} \in M_n$ be given.  We say that $\matr{A}$ is \textnormal{unitarily similar} to $\matr{B}$ if there is a unitary
  $\matr{U} \in M_n$ such that $\matr{A} = \matr{U}\matr{B}\matr{U}^*$.
\end{defn}

\begin{proof}[Singular Value Decomposition]
  It should be clear the matrices $\matr{A}\matr{A}^* \in M_n$ and $\matr{A}^*\matr{A} \in M_n$ have the same eigenvalues, and hence, they are
  unitarily similar.  Then there exists a unitary matrix $\matr{U}$ such that $\matr{A}^*\matr{A} = \matr{U}(\matr{A}\matr{A}^*)\matr{U}^*$.  Then

  \[
    {(\matr{UA})}^*(\matr{UA}) =
    \matr{A}^*\matr{U}^*\matr{UA} =
    \matr{A}^*\matr{A} =
    \matr{UA}\matr{A}^*\matr{U}^* =
    \matr{UA}{(\matr{U}\matr{A})}^*
  \]

  so $\matr{UA}$ is normal.  Let $\lambda_1 = \abs{\lambda_1}e^{i\theta_1}, \ldots, \lambda_n = \abs{\lambda_n}e^{i\theta_n}$ be the positive eigenvalues of
  $\matr{UA}$ in decreasing order.  Furthermore, let $\Delta = diag(\lambda_1, \ldots, \lambda_n)$, let $D = diag(e^{i\theta_1}, \ldots, e^{i\theta_n})$,
  let $\Sigma = diag(\abs{\lambda_1}, \ldots, \abs{\lambda_n})$, and let $\matr{X}$ be a unitary matrix such that $\matr{UA} = \matr{X\Delta}\matr{X}^*$.  Then
  $D$ is unitary and

  \[
    \matr{A} = \matr{U}^*\matr{X}\Sigma\matr{D}\matr{X}^* = (\matr{U}^*\matr{X})\Sigma(\matr{D}\matr{X}^*)
  \]

  If we denote $\matr{V} = \matr{U}^*\matr{X}$ and $\matr{W} = \matr{X}\matr{D}^*$, we have our desired factorization, and
  $\sigma_j = \abs{\lambda_j}, j = 1, \ldots, n$.
\end{proof}

A full proof of \gls{SVD} for rectangular matrices can be found in Horn and Johnson\cite{horn2013}. The relationship between
\gls{SVD} and \gls{PCA} follows directly from the definition of \gls{SVD}.

\begin{thm}
  Let $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$ be the \gls{SVD} of an $n \times n$ dimensional matrix $\matr{A}$ and let

  \[
    \matr{C} = \frac{1}{n - 1}\matr{A}^*\matr{A}
  \]

  be the covariance matrix.  The eigenvectors of $\matr{C}$ are the same as the \textnormal{right singular vectors} of
  $\matr{\Sigma}$.
\end{thm}

\begin{proof}
  Compute
  \[
    \matr{A}^*\matr{A} =
    \matr{V\Sigma}\matr{W}^*\matr{W\Sigma}\matr{V}^* =
    \matr{V\Sigma\Sigma}\matr{V}^* =
    \matr{V}\matr{\Sigma}^2\matr{V}^*
  \]

  \[
    \matr{C} = \matr{V}\frac{\matr{\Sigma}^2}{n - 1}\matr{V}^*
  \]

  $\matr{C}$ is symmetric, and unitarily diagnolizable.  Hence, the eigenvectors of the covariance matrix $\matr{C}$ are the same as the
  matrix $\matr{V}$ (right singular vectors) and the eigenvalues of $\matr{C}$ can be computed directly from the singular values
  $\lambda_i = \frac{\sigma_i}{n - 1}$.
\end{proof}

In principal component analysis, we call the ordered eigenvectors \textit{principal components}.  To ascertain the importance of each
component, one must analyze the \gls{eigenspectrum}, usually in the form of a \gls{scree plot}.  Conceptually, the normalized size of a component's
associated eigenvalue conveys the amount of variation that component captures.  A researcher can the choose a suitable subset of components to
analyze, reducing the dimensionality of the data set the chosen components.

\section*{Detecting changes in local chromatin interaction}

\gls{PCA} is a useful technique for identifying gross differences between data sets.  For subtle changes of local chromatin structure, we appropriate a
technique developed by Bin Ren and colleagues\cite{ren2013}, termed the \gls{DI}.  Intuitively, the \gls{DI} for a given region of chromatin gives the
relative `upstream' or `downstream' character of the interactions involving that particular chromatin region.

The basic premise of the \gls{DI} algorithm is simple.  For each bin along the genome, we calculate the number of upstream and downstream interactions and
assign the normalized difference ($downstream - upstream$) as the \gls{DI} for that particular bin.  The index captures the downstream or upstream bias of
a genomic region. The formulation of the directionality index is given in the supplementary methods of Ren's paper\cite{ren2013}

\[
  DI = (\frac{B - A}{\abs{B - A}})(\frac{{(A - E)}^2}{E} + \frac{{(B - E)}^2}{E})
\]

where $A$ is the number of upstream reads in a given window, $B$ is the number of downstream reads, and $E$ is the expected number of interactions under
the null distribution ($\frac{A + B}{2}$).  Ren and colleagues apply the directionality index to examine 2Mb widows upstream and downstream on a normalized contact map at 40Kb resolution.
