% math.tex

\chapter{Analytical Preliminaries}

In this thesis, we endeavor to secure a relationship between chromatin structure and genomic fragility, to help understand the
progression towards disease states.  Unfortunately, the mathematical tools we will leverage, while both appropriate and necessary,
do not flow naturally from one mathematical field.  Hence, the following chapter will feel disjointed.  By the end, we will have
dabbled in linear algebra, numerical optimization, probability and statistics.

\section*{Normalization of Chromatin Contact Maps}

The entry point for chromatin interaction analysis begins with a chromatin \gls{contact map}.  The Hi-C experiment generates a
genomic contact map describing the number of observed contacts between any two genomic regions, the details of which are covered
in Chapter 2.  Contacts are recorded in square matrix $\matr{O}$ by binning the genome into equally sized intervals.  These bins
are ordered by increasing genomic coordinates, from the start of chromosome 1 to the last bin of chromosome Y.  The cell $O_{ij}$
records the number of observed interactions between bins $i$ and $j$.  Depending on the data quality, bin sizes range from tens of
kilobases to megabases.

The first step of data analysis is remove experimental biases and establish reproducibility.  Several methods have been proposed to
normalize chromatin maps.  Tanay and Yaffe perform a robust analysis of the Hi-C assay identifying several sources systematic
biases\cite{tanay2011}. They propose a non-parametric model to normalize contact maps by focusing on three systematic biases: variable
genome mappability, \gls{GC} content, and variable fragment length.


% TODO


\section*{Gene Expression Profiling}

The set of genes expressed in a cell, collectively known as the \textit{transcriptome}, provides insight into chromatin accessibility
and cell state.  A gene expressed in $n$ conditions (or $n$ samples) forms vector in $n-$dimensional space.  Similarly, the expression
profile the entire transcriptome for a single sample is a vector in $m-$dimensional space, where $m$ is the number of genes assessed.
We therefore denote an experiment interrogating $n$ genes across $m$ samples as an $m \times n$ matrix $S_{m,n}$.
\[
  S_{m,n} = \left[
    \begin{array}{cccc}
      s_{1,1} & s_{1,2} & \cdots & s_{1,n} \\
      s_{2,1} & s_{2,2} & \cdots & s_{2,n} \\
      \vdots & \vdots & \ddots  & \vdots \\
      s_{m,1} & s_{m,2} & \cdots & s_{m,n}
    \end{array}
  \right]
\]

and a single gene's expression profile is
\[ G_{i} = \begin{pmatrix} g_{i1} & g_{i2} & g_{i3} & \cdots & g_{in} & \end{pmatrix} \]

We wish to compare differences in gene expression profiles for particular, or all, genes between samples.  This amounts to comparing
\textit{distances} between gene expression vectors.  There are several methods for doing so.

\subsection*{Euclidean Distance}

The simplest genomic distance measure is \textbf{Euclidean} distance.  Euclidean distance is a particular \textit{norm}, known as
the 2-norm.  Geometrically, the norm is a measure of `size' or `proximity' in $n-$dimensional space.  The 2-norm for a vector $x$ is
defined as
\[
  \|x\|_{2} = {(|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)}^{\frac{1}{2}}
\]

The distance between two vectors in a vector space can now be found using the triangle inequality on the 2-norm.  As a norm, the Euclidean
norm has some desirable properties, notably, non-negativity, positivity, and a definition of the triangle inequality
$\|x + y\| = \|x\| + \|y\|$~\cite{horn2013}.  Directly from the triangle inequality, we define the Euclidean distance in $n-$dimensions to
be
\[
  D_{Euc}(x,y) = \|x\| - \|y\| = {(\sum_{i = 1}^{n}{(a_i - b_i)}^2)}^{\frac{1}{2}}
\]

\subsection*{Correlations between Vectors}

\textbf{TODO}

\section*{Iterative Correction of Interaction Maps}

Massive data mining experiments must be cognizant of any inherent biases in the data set and seek to eliminate them prior to performing
meaningful data analysis.  In this thesis, we depend on accurate reporting of contact frequency between genomic loci, which is prone
to a number of experimental, mechanical, and biological biases\cite{dekker2006}.  In order to compare data sets, I employ an algorithm,
iterative correction and eigenvector decomposition (ICE) developed by Imakaev and colleagues\cite{imakaev2012} established on previous
work by Tanay and Yaffe\cite{yaffe2011}.  In the following section, I will describe the motivation for ICE, the algorithm in detail, and
prove convergence properties for the algorithm.

Genome-wide interaction maps form matrix of observed contact frequencies between genomic segments.  Observed contacts are expected to
be noisy do to biases introduced in the experimental procedure and inherent differences in mappability for different genomic regions
(e.g.\ centromeres).

% Why are biases factorizable?

The ICE algorithm decomposes a contact map into the product of true contact frequencies and a set of biases for every region
\[
  O_{ij} = B_{i}B_{j}T_{ij}
\]
with $T_{ij}$ normalized as $\sum_{i,i \neq j,j \pm 1}T_{ij} = 1$ for each region $j$.




\section*{Principal Component Analysis}

The holy grail of data analysis on high-dimensional data is dimensionality reduction --- that is, to find an accurate representation of
the experiment that need not invoke all the dimensions measured.  The benefits of preprocessing the data set into fewer dimensions are
increases in storage capacity and analysis speed.  As experiments such as Hi-C produce large and larger quantities of data, researchers
attempt to find ways to remove redundency, eliminate unneeded parameters and compress datasets.  One of the most popular methods is
dalled \gls{PCA}\cite{law1987}.

\begin{defn}[Principal Component Analysis]
  A statistical proceedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables.
\end{defn}

In practice, there are two methods used for \gls{PCA}.  The simplest to explain, but more error-prone, is the eigen-decompositon
method\cite{}.  In this proceedure, the eigenvalues of the covarience matrix $\matr{A}\matr{A}^T$ are calculated directly.  However,
because this involves an extra matrix multiplication, numerical errors are more likely to be introduced during large computations.
In practice, \gls{PCA} often derived in conjunction with \gls{SVD} and we will hold to that standard here.

\begin{thm}[Singular Value Decomposition]
  Let $\matr{A} \in M_{n}(\mathbb{R})$ be given. Then there are unitary matrices $\matr{V} \in M_n$ and $\matr{W} \in M_n$, and a square diagonal
  matrix
  \[
    \matr{\Sigma} =
      \begin{bmatrix}
        \sigma_1 &        & 0        \\
                 & \ddots &          \\
        0        &        & \sigma_n \\
      \end{bmatrix}
  \]
  such that $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n$ and $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$.  The parameters $\sigma_1$,
  $\hdots$, $\sigma_n$ are the positive square roots of the decreasingly ordered non-zero eigenvalues of $\matr{A}\matr{A}^*$, which are the
  same as the decreasingly ordered nonzero eigenvalues of $\matr{A}^*\matr{A}$.

\end{thm}

In order to prove that any square matrix $\matr{A} \in M_n(\mathbb{R})$ can be decomposed into singular values, we must first recall some
matrix definitions.

% Hermetian
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{Hermetian} if $\matr{A}^* =  \matr{A}$.  For real matrices, symmetry implies the Hermetian property.
\end{defn}

% normal
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{normal} if $\matr{A}\matr{A}^* = \matr{A}^*\matr{A}$, this is, it commutes with its conjugate transpose.
\end{defn}

% unitary
\begin{defn}
  A matrix $\matr{A} \in M_n$ is \textnormal{unitary} if $\matr{A}^*\matr{A} = \matr{A}\matr{A}^* = 1$.  Equivalently, $\matr{A}^* = \matr{A}^{-1}$.
\end{defn}

% unitarily similar
\begin{defn}
  Let $\matr{A},\matr{B} \in M_n$ be given.  We say that $\matr{A}$ is \textnormal{unitarily similar} to $\matr{B}$ if there is a unitary
  $\matr{U} \in M_n$ such that $\matr{A} = \matr{U}\matr{B}\matr{U}^*$.
\end{defn}

\begin{proof}[Singular Value Decomposition]
  It should be clear the matrices $\matr{A}\matr{A}^* \in M_n$ and $\matr{A}^*\matr{A} \in M_n$ have the same eigenvalues, and hence, they are
  unitarily similar.  Then there exists a unitary matrix $\matr{U}$ such that $\matr{A}^*\matr{A} = \matr{U}(\matr{A}\matr{A}^*)\matr{U}^*$.  Then

  \[
    {(\matr{UA})}^*(\matr{UA}) =
    \matr{A}^*\matr{U}^*\matr{UA} =
    \matr{A}^*\matr{A} =
    \matr{UA}\matr{A}^*\matr{U}^* =
    \matr{UA}{(\matr{U}\matr{A})}^*
  \]

  so $\matr{UA}$ is normal.  Let $\lambda_1 = \abs{\lambda_1}e^{i\theta_1}, \ldots, \lambda_n = \abs{\lambda_n}e^{i\theta_n}$ be the positive eigenvalues of
  $\matr{UA}$ in decreasing order.  Furthermore, let $\Delta = diag(\lambda_1, \ldots, \lambda_n)$, let $D = diag(e^{i\theta_1}, \ldots, e^{i\theta_n})$,
  let $\Sigma = diag(\abs{\lambda_1}, \ldots, \abs{\lambda_n})$, and let $\matr{X}$ be a unitary matrix such that $\matr{UA} = \matr{X\Delta}\matr{X}^*$.  Then
  $D$ is unitary and

  \[
    \matr{A} = \matr{U}^*\matr{X}\Sigma\matr{D}\matr{X}^* = (\matr{U}^*\matr{X})\Sigma(\matr{D}\matr{X}^*)
  \]

  If we denote $\matr{V} = \matr{U}^*\matr{X}$ and $\matr{W} = \matr{X}\matr{D}^*$, we have our desired factorization, and
  $\sigma_j = \abs{\lambda_j}, j = 1, \ldots, n$.
\end{proof}

A full proof of \gls{SVD} for rectangular matrices can be found in Horn and Johnson\cite{horn2013}. The relationship between
\gls{SVD} and \gls{PCA} follows directly from the definition of \gls{SVD}.

\begin{thm}
  Let $\matr{A} = \matr{V}\matr{\Sigma}\matr{W}^*$ be the \gls{SVD} of an $n \times n$ dimensional matrix $\matr{A}$ and let

  \[
    \matr{C} = \frac{1}{n - 1}\matr{A}^*\matr{A}
  \]

  be the covariance matrix.  The eigenvectors of $\matr{C}$ are the same as the \textnormal{right singular vectors} of
  $\matr{\Sigma}$.
\end{thm}

\begin{proof}
  Compute
  \[
    \matr{A}^*\matr{A} =
    \matr{V\Sigma}\matr{W}^*\matr{W\Sigma}\matr{V}^* =
    \matr{V\Sigma\Sigma}\matr{V}^* =
    \matr{V}\matr{\Sigma}^2\matr{V}^*
  \]

  \[
    \matr{C} = \matr{V}\frac{\matr{\Sigma}^2}{n - 1}\matr{V}^*
  \]

  $\matr{C}$ is symmetric, and unitarily diagnolizable.  Hence, the eigenvectors of the covariance matrix $\matr{C}$ are the same as the
  matrix $\matr{V}$ (right singular vectors) and the eigenvalues of $\matr{C}$ can be computed directly from the singular values
  $\lambda_i = \frac{\sigma_i}{n - 1}$.
\end{proof}

In principal component analysis, we call the ordered eigenvectors \textit{principal components}.  To ascertain the importance of each
component, one must analyze the \gls{eigenspectrum}, usually in the form of a \gls{scree plot}.  Conceptually, the normalized size of a component's
associated eigenvalue conveys the amount of variation that component captures.  A researcher can the choose a suitable subset of components to
analyze, reducing the dimensionality of the dataset the chosen components.

